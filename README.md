# Allure Failure Analyzer

> A fast, configurable CLI that scans **Allure** results, **groups failures by root cause**, and produces an easy‑to‑read **interactive HTML** dashboard (plus JSON/Markdown summary).

![alt text](image.png)

---

## Why this README is tailored
This README reflects **your current repo setup**:
- `config.yaml` currently defines `allure_results_directory`, `output_report_file`, and `top_n_groups_to_report`.
- `report.html` loads **`failure_analysis_report.json`** via `fetch(...)` (so the JSON file name/path must match).
- `requirements.txt` contains only `PyYAML`.
- The generated JSON file (`failure_analysis_report.json`) includes a `metadata` block and an array of `groups`.

> If you later add further config keys (e.g., `project_root_package`), paste them under **Configuration**. The tool still works if that key is omitted; in TS/JS projects the analyzer falls back to step names/labels when no code location is found.

---

## Table of Contents
- [Project Structure](#project-structure)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [Outputs](#outputs)
- [Behavior nuances & defaults](#behavior-nuances--defaults)
- [How It Works](#how-it-works)
- [Tips](#tips)
- [Troubleshooting](#troubleshooting)
- [FAQ](#faq)
- [License](#license)

---

## Project Structure
```text
allure-analyzer/
├─ analyzer/
│  ├─ __init__.py
│  ├─ ingestion.py
│  ├─ fingerprinter.py
│  └─ reporting.py
├─ config.yaml
├─ main.py
├─ report.html
├─ failure_analysis_report.json        # generated by the tool
└─ requirements.txt
```

---

## Prerequisites
- **Python** 3.8+
- **pip** (Python package manager)
- Allure **results** folder from your CI/local run (usually `allure-results/` with many `*-result.json` files).

---

## Installation
```bash
# (Recommended) create and activate a virtual environment
python -m venv .venv
# macOS/Linux
source .venv/bin/activate
# Windows (PowerShell)
# .venv\Scripts\Activate.ps1

# install dependencies (minimal)
pip install -r requirements.txt
```

---

## Configuration
Your current `config.yaml` contains these keys (edit as needed):

```yaml
# Path to the Allure results directory generated after a test run.
allure_results_directory: './allure-results'

# Name of the interactive HTML report file (used as the main report artifact name).
# NOTE: The static UI lives in 'report.html' (see IMPORTANT note below).
output_report_file: 'failure_analysis_report.html'

# The number of top failure groups to include in summaries (use 0 or negative to include all).
top_n_groups_to_report: 20
```

**IMPORTANT (about the HTML & JSON pairing):**
- The dashboard HTML file is `report.html` (static). It **expects** to load `failure_analysis_report.json` from the same directory.
- If you rename/move the JSON file, you must also update the `fetch('failure_analysis_report.json')` call in `report.html` accordingly.
- Keep `report.html` and the generated `failure_analysis_report.json` together when publishing/serving the report.

> (Optional, recommended) You can add this key for better stack-location grouping in non-Java projects (TS/JS):
> ```yaml
> # Used to locate the most relevant line in stack traces (first match wins).
> # For TypeScript/JS projects, set a stable code path segment, e.g., 'src/', 'apps/web', 'packages/'.
> project_root_package: 'src/'
> ```

---

## Usage
Run the analyzer from the project root:
```bash
python main.py
```

What happens:
1. The tool scans the folder from `allure_results_directory` for `*-result.json` files (and related Allure structures).
2. It extracts all **failed**/**broken** failures (including nested steps & attachments).
3. It creates **groups** by fingerprint (normalized message/step + best code location).
4. It writes artifacts:
   - `failure_analysis_report.json` (canonical data for the dashboard)
   - `failure_analysis_report.md` (Markdown summary; if enabled by your current code)
   - `report.html` (static UI, which loads the JSON at runtime)

Open the dashboard locally:
```bash
# option A: with the built-in prompt in main.py (y/n)
python main.py

# option B: simple local server
python -m http.server
# then open http://localhost:8000/report.html
```

---

## Outputs
- **JSON** — `failure_analysis_report.json` with a `metadata` block and an array of `groups` (each has id, title, counts, fingerprint, labels, and an example with message/trace).
- **HTML** — `report.html`, the interactive dashboard that reads the JSON via `fetch(...)`.
- **Markdown** — `failure_analysis_report.md`, a human‑readable summary (if enabled by your current code).

> Keep the HTML and JSON files together when uploading/downloading artifacts so the viewer can load data.

---

## Behavior nuances & defaults
- **Counting model**: totals in the dashboard reflect **failure steps** (including nested steps), **not test count**. Allure’s native summary counts tests. This is by design.
- **Top‑N**: header totals reflect only the **shown groups** (after Top‑N slicing). Set `top_n_groups_to_report: 0` or any negative value to include **all** groups.
- **Titles & messages**:
  - Messages that start with `Custom message:` are **flattened** (newlines collapsed) so titles are informative (e.g., `Custom message: Expected ...`).
  - Very long messages (e.g., CSP violations) are **shortened** with dedicated rules to keep titles clean.
  - Titles are trimmed to **160 characters** by default (`MAX_TITLE_LEN` in `analyzer/fingerprinter.py`).

**JSON contract used by the dashboard**
- `metadata`: `generation_date`, `total_failures`, `unique_groups`
- `groups[]`:
  - `id`, `title`, `count`, `percentage`
  - `status_counts.{failed, broken}`
  - `fingerprint_what`, `fingerprint_where`
  - `epics`, `features`
  - `example.{test_name, message, trace}`

---

## How It Works
**Ingestion (`analyzer/ingestion.py`)**
- Parallel parsing of Allure JSON (`*-result.json`, with correlation to containers/steps/attachments when present).
- Extracts robust **message** and **trace**, plus labels (epic/feature/etc.) and failing step.

**Fingerprinting (`analyzer/fingerprinter.py`)**
- Normalizes dynamic tokens (UUIDs, long numbers, IDs).
- Prefers **failing step name/message** as the “what”, and a relevant **code location** as the “where” (fallbacks exist for TS/JS traces).
- Dedicated rules keep titles short for certain patterns (e.g., CSP), while `Custom message:` is flattened for readability.

**Reporting (`analyzer/reporting.py`)**
- Groups failures by fingerprint and sorts by frequency.
- Aggregates **epics**/**features** impacted.
- Writes the JSON/Markdown artifacts; the static `report.html` renders the JSON client‑side.

---

## Tips
- Place `allure_results_directory` on a fast disk (SSD) for large runs.
- For TypeScript/Playwright projects, set `project_root_package: 'src/'` (or similar) to improve “where” accuracy.
- If you store stacks inside custom JSON/text attachments, extend extraction rules in `ingestion.py`.

---

## Troubleshooting
- **“Failed to load report data” in the browser** — Serve via a local web server (e.g., `python -m http.server`) so `fetch(...)` can load the JSON.
- **Too many unique groups** — Add `project_root_package` and/or refine normalization rules in `fingerprinter.py`.
- **No files found** — Ensure the `allure_results_directory` path is correct and contains `*-result.json` files.

---

## FAQ
**Does the tool make external calls?**  
No. It is fully offline by design.

**Does it work for TypeScript/Playwright/Cypress?**  
Yes. It extracts errors from attachments and falls back to labels when stacks are missing.

**Can I customize fingerprint rules?**  
Yes. Edit `analyzer/fingerprinter.py` to add/adjust normalization patterns and selection priority.

---

## License
[MIT](https://github.com/keinar/AI-Bug-Reporter/blob/main/LICENSE)

